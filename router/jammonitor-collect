#!/bin/sh
# JamMonitor Historical Metrics Collector v4
# Collects metrics every MINUTE (not second) to minimize storage
# 24h = ~1,440 entries (~300KB) vs previous ~86,400 entries (~18MB)

PIDFILE="/var/run/jammonitor-collect.pid"

# Prevent multiple instances
if [ -f "$PIDFILE" ]; then
    OLDPID=$(cat "$PIDFILE" 2>/dev/null)
    if [ -n "$OLDPID" ] && kill -0 "$OLDPID" 2>/dev/null; then
        logger -t jammonitor "Collector already running (PID $OLDPID), exiting"
        exit 0
    fi
    # Stale PID file, remove it
    rm -f "$PIDFILE"
fi

# Write our PID
echo $$ > "$PIDFILE"

# Cleanup PID file on exit
trap "rm -f '$PIDFILE'" EXIT INT TERM

DB_PATH="/mnt/data/jammonitor/history.db"
LOG_PATH="/mnt/data/jammonitor/syslog.txt"
CLIENT_SNAPSHOT="/tmp/jammonitor_client_last"
RETENTION_DAYS=30
RETENTION_DAYS_RAW=30      # Keep raw per-client data for 30 days
DISK_THRESHOLD=80          # Start cleanup when disk usage exceeds this %
MAX_LOG_SIZE=104857600     # 100MB max syslog size
WAN_CONFIG="/etc/jammonitor_wans"

# Ensure data directory exists
mkdir -p /mnt/data/jammonitor 2>/dev/null || {
    logger -t jammonitor "FATAL: Cannot create data directory /mnt/data/jammonitor"
    exit 1
}

# Helper: Escape single quotes for SQLite (double them)
sql_escape() {
    printf '%s' "$1" | sed "s/'/''/g"
}

# Get WAN interface list (dynamic or fallback)
get_wan_list() {
    if [ -f "$WAN_CONFIG" ]; then
        cat "$WAN_CONFIG" | tr '\n' ' '
    else
        echo "wan wan1 wan2 wan3 wan4"
    fi
}

# Initialize database with all tables and WAL mode
init_db() {
    sqlite3 "$DB_PATH" "
        PRAGMA journal_mode=WAL;
        PRAGMA busy_timeout=5000;
        CREATE TABLE IF NOT EXISTS metrics (
            ts INTEGER PRIMARY KEY,
            load TEXT,
            ram_pct REAL,
            temp INTEGER,
            wan_pings TEXT,
            iface_status TEXT
        );
        CREATE TABLE IF NOT EXISTS snapshots (
            ts INTEGER PRIMARY KEY,
            mptcp TEXT,
            vpn TEXT,
            routes TEXT,
            conntrack_count INTEGER,
            dns TEXT
        );
        CREATE TABLE IF NOT EXISTS client_traffic (
            ts INTEGER,
            ip TEXT,
            mac TEXT,
            hostname TEXT,
            rx_bytes INTEGER,
            tx_bytes INTEGER,
            PRIMARY KEY (ts, ip)
        );
        CREATE TABLE IF NOT EXISTS client_traffic_hourly (
            hour_ts INTEGER,
            ip TEXT,
            mac TEXT,
            hostname TEXT,
            rx_bytes INTEGER,
            tx_bytes INTEGER,
            PRIMARY KEY (hour_ts, ip)
        );
        CREATE TABLE IF NOT EXISTS interface_traffic (
            ts INTEGER,
            iface TEXT,
            rx_bytes INTEGER,
            tx_bytes INTEGER,
            PRIMARY KEY (ts, iface)
        );
        CREATE INDEX IF NOT EXISTS idx_metrics_ts ON metrics(ts);
        CREATE INDEX IF NOT EXISTS idx_snapshots_ts ON snapshots(ts);
        CREATE INDEX IF NOT EXISTS idx_client_ts ON client_traffic(ts);
        CREATE INDEX IF NOT EXISTS idx_client_hourly_ts ON client_traffic_hourly(hour_ts);
        CREATE INDEX IF NOT EXISTS idx_iface_ts ON interface_traffic(ts);
    "
}

# Collect metrics (every minute)
collect_metrics() {
    TS=$(date +%s)
    LOAD=$(cat /proc/loadavg | cut -d' ' -f1-3)
    RAM=$(free | awk '/Mem:/{printf "%.1f", $3/$2*100}')
    TEMP=$(cat /sys/class/thermal/thermal_zone0/temp 2>/dev/null || echo 0)

    # Pings with timeout protection
    P1=$(timeout 2 ping -c1 -W1 1.1.1.1 2>/dev/null | grep -oE 'time=[0-9.]+' | cut -d= -f2)
    P2=$(timeout 2 ping -c1 -W1 8.8.8.8 2>/dev/null | grep -oE 'time=[0-9.]+' | cut -d= -f2)
    [ -z "$P1" ] && P1="null"
    [ -z "$P2" ] && P2="null"
    PINGS="{\"1.1.1.1\":$P1,\"8.8.8.8\":$P2}"

    # Interface status (dynamic list)
    IFACES="{"
    FIRST=1
    for iface in $(get_wan_list); do
        UP=$(timeout 2 ifstatus $iface 2>/dev/null | jsonfilter -e '@.up' 2>/dev/null)
        [ $FIRST -eq 0 ] && IFACES="$IFACES,"
        [ "$UP" = "true" ] && IFACES="$IFACES\"$iface\":1" || IFACES="$IFACES\"$iface\":0"
        FIRST=0
    done
    IFACES="$IFACES}"

    # Escape values for SQL safety
    LOAD_ESC=$(sql_escape "$LOAD")
    PINGS_ESC=$(sql_escape "$PINGS")
    IFACES_ESC=$(sql_escape "$IFACES")

    sqlite3 "$DB_PATH" "INSERT OR REPLACE INTO metrics VALUES ($TS, '$LOAD_ESC', $RAM, $TEMP, '$PINGS_ESC', '$IFACES_ESC');" 2>/dev/null
}

# Collect slow snapshots (every minute)
collect_slow() {
    TS=$(date +%s)

    # MPTCP endpoints and limits (with timeout)
    MPTCP_EP=$(timeout 5 ip mptcp endpoint show 2>/dev/null | head -20 | tr '\n' '|')
    MPTCP_LIM=$(timeout 2 ip mptcp limits 2>/dev/null | tr '\n' '|')
    MPTCP="$MPTCP_EP;;$MPTCP_LIM"

    # VPN status (simplified)
    VPN=""
    if command -v wg >/dev/null 2>&1; then
        VPN=$(timeout 2 wg show interfaces 2>/dev/null | tr '\n' ',')
    fi
    GT=$(pgrep -c glorytun 2>/dev/null || echo 0)
    OV=$(pgrep -c openvpn 2>/dev/null || echo 0)
    VPN="wg:$VPN|gt:$GT|ovpn:$OV"

    # Routes summary (default routes only)
    ROUTES=$(ip route show default 2>/dev/null | head -10 | tr '\n' '|')

    # Conntrack count
    CT_COUNT=$(cat /proc/sys/net/netfilter/nf_conntrack_count 2>/dev/null || echo 0)

    # DNS servers
    DNS=$(grep nameserver /tmp/resolv.conf 2>/dev/null | awk '{print $2}' | tr '\n' ',' | sed 's/,$//')

    # Escape all values for SQL safety
    MPTCP_ESC=$(sql_escape "$MPTCP")
    VPN_ESC=$(sql_escape "$VPN")
    ROUTES_ESC=$(sql_escape "$ROUTES")
    DNS_ESC=$(sql_escape "$DNS")

    sqlite3 "$DB_PATH" "INSERT OR REPLACE INTO snapshots VALUES ($TS, '$MPTCP_ESC', '$VPN_ESC', '$ROUTES_ESC', $CT_COUNT, '$DNS_ESC');" 2>/dev/null
}

# Collect per-client traffic (every minute)
# Tracks delta bytes per IP since last sample
collect_client_traffic() {
    TS=$(date +%s)
    CURRENT_FILE="${CLIENT_SNAPSHOT}.current"
    LAST_FILE="${CLIENT_SNAPSHOT}.last"

    # Build MAC->hostname lookup from DHCP leases
    # Format: timestamp mac ip hostname clientid
    HOSTNAME_MAP=""
    if [ -f /tmp/dhcp.leases ]; then
        while read -r exp mac ip host rest; do
            [ -n "$mac" ] && [ -n "$host" ] && [ "$host" != "*" ] && \
                HOSTNAME_MAP="${HOSTNAME_MAP}${mac}=${host}\n"
        done < /tmp/dhcp.leases
    fi

    # Build IP->MAC lookup from ARP table
    MAC_MAP=""
    while read -r ip type flags mac mask iface; do
        [ "$ip" = "IP" ] && continue  # Skip header
        [ -n "$ip" ] && [ -n "$mac" ] && [ "$mac" != "00:00:00:00:00:00" ] && \
            MAC_MAP="${MAC_MAP}${ip}=${mac}\n"
    done < /proc/net/arp

    # Parse conntrack and aggregate bytes per source IP
    # conntrack format: ... src=IP ... bytes=X ... bytes=Y ...
    # First bytes is TX (outgoing), second is RX (incoming)
    > "$CURRENT_FILE"
    conntrack -L 2>/dev/null | head -1000 | while read -r line; do
        # Extract source IP (local client)
        src=$(echo "$line" | grep -oE 'src=[0-9.]+' | head -1 | cut -d= -f2)
        [ -z "$src" ] && continue

        # Skip non-local IPs (external sources)
        # Include: RFC1918 private ranges + Tailscale (100.x.x.x)
        case "$src" in
            10.*|192.168.*|172.1[6-9].*|172.2[0-9].*|172.3[0-1].*|100.*) ;;
            *) continue ;;
        esac

        # Extract bytes (first=TX, second=RX from client perspective)
        bytes=$(echo "$line" | grep -oE 'bytes=[0-9]+' | cut -d= -f2 | tr '\n' ' ')
        tx=$(echo "$bytes" | awk '{print $1}')
        rx=$(echo "$bytes" | awk '{print $2}')
        [ -z "$tx" ] && tx=0
        [ -z "$rx" ] && rx=0

        echo "${src} ${tx} ${rx}" >> "$CURRENT_FILE"
    done

    # Aggregate by IP (sum all entries for same IP)
    AGGREGATED_FILE="${CLIENT_SNAPSHOT}.agg"
    awk '{
        ip=$1; tx=$2; rx=$3
        total_tx[ip] += tx
        total_rx[ip] += rx
    } END {
        for (ip in total_tx) {
            print ip, total_tx[ip], total_rx[ip]
        }
    }' "$CURRENT_FILE" > "$AGGREGATED_FILE" 2>/dev/null

    # Compare with last snapshot to get deltas
    if [ -f "$LAST_FILE" ]; then
        while read -r ip tx rx; do
            [ -z "$ip" ] && continue

            # Get previous values
            prev=$(grep "^${ip} " "$LAST_FILE" 2>/dev/null | head -1)
            if [ -n "$prev" ]; then
                prev_tx=$(echo "$prev" | awk '{print $2}')
                prev_rx=$(echo "$prev" | awk '{print $3}')
            else
                prev_tx=0
                prev_rx=0
            fi

            # Calculate delta (handle counter reset)
            delta_tx=$((tx - prev_tx))
            delta_rx=$((rx - prev_rx))
            [ $delta_tx -lt 0 ] && delta_tx=$tx
            [ $delta_rx -lt 0 ] && delta_rx=$rx

            # Skip if no traffic
            [ $delta_tx -eq 0 ] && [ $delta_rx -eq 0 ] && continue

            # Resolve MAC and hostname
            mac=$(printf "$MAC_MAP" | grep "^${ip}=" | head -1 | cut -d= -f2)
            [ -z "$mac" ] && mac="unknown"
            hostname="*"
            if [ "$mac" != "unknown" ]; then
                hostname=$(printf "$HOSTNAME_MAP" | grep -i "^${mac}=" | head -1 | cut -d= -f2)
                [ -z "$hostname" ] && hostname="*"
            fi

            # Escape for SQL
            ip_esc=$(sql_escape "$ip")
            mac_esc=$(sql_escape "$mac")
            host_esc=$(sql_escape "$hostname")

            # Insert into database
            sqlite3 "$DB_PATH" "INSERT OR REPLACE INTO client_traffic VALUES ($TS, '$ip_esc', '$mac_esc', '$host_esc', $delta_rx, $delta_tx);" 2>/dev/null

        done < "$AGGREGATED_FILE"
    fi

    # Save current as last for next iteration
    mv "$AGGREGATED_FILE" "$LAST_FILE" 2>/dev/null
    rm -f "$CURRENT_FILE" 2>/dev/null
}

# Collect interface-level traffic totals (every minute)
# Used to calculate unattributed traffic (total - client sum)
collect_interface_traffic() {
    TS=$(date +%s)
    IFACE_SNAPSHOT="${DATA_DIR}/iface_snapshot"
    CURRENT_IFACE="${IFACE_SNAPSHOT}.current"
    LAST_IFACE="${IFACE_SNAPSHOT}.last"

    # Collect current interface byte counters
    > "$CURRENT_IFACE"
    for iface in $(ls /sys/class/net/ 2>/dev/null); do
        # Skip loopback and virtual interfaces we don't care about
        case "$iface" in
            lo|docker*|veth*|br-*) continue ;;
        esac
        RX=$(cat /sys/class/net/$iface/statistics/rx_bytes 2>/dev/null || echo 0)
        TX=$(cat /sys/class/net/$iface/statistics/tx_bytes 2>/dev/null || echo 0)
        echo "$iface $RX $TX" >> "$CURRENT_IFACE"
    done

    # If we have a previous snapshot, calculate deltas
    if [ -f "$LAST_IFACE" ]; then
        while read -r iface curr_rx curr_tx; do
            # Find this interface in last snapshot
            last_line=$(grep "^$iface " "$LAST_IFACE" 2>/dev/null)
            if [ -n "$last_line" ]; then
                last_rx=$(echo "$last_line" | awk '{print $2}')
                last_tx=$(echo "$last_line" | awk '{print $3}')

                # Calculate delta (handle counter wrap)
                delta_rx=$((curr_rx - last_rx))
                delta_tx=$((curr_tx - last_tx))
                [ "$delta_rx" -lt 0 ] && delta_rx=0
                [ "$delta_tx" -lt 0 ] && delta_tx=0

                # Store if non-zero
                if [ "$delta_rx" -gt 0 ] || [ "$delta_tx" -gt 0 ]; then
                    sqlite3 "$DB_PATH" "INSERT OR REPLACE INTO interface_traffic (ts, iface, rx_bytes, tx_bytes) VALUES ($TS, '$iface', $delta_rx, $delta_tx);" 2>/dev/null
                fi
            fi
        done < "$CURRENT_IFACE"
    fi

    mv "$CURRENT_IFACE" "$LAST_IFACE" 2>/dev/null
}

# Stream syslog (continuous append with rotation)
stream_syslog() {
    # Rotate if too big
    if [ -f "$LOG_PATH" ]; then
        SIZE=$(stat -c%s "$LOG_PATH" 2>/dev/null || stat -f%z "$LOG_PATH" 2>/dev/null || echo 0)
        if [ "$SIZE" -gt "$MAX_LOG_SIZE" ]; then
            mv "$LOG_PATH" "${LOG_PATH}.old"
        fi
    fi

    # Append recent syslog entries (last 100 lines since last check)
    logread 2>/dev/null | tail -100 >> "$LOG_PATH"
}

# Rollup old per-client traffic to hourly summaries
rollup_client_traffic() {
    CUTOFF_RAW=$(($(date +%s) - RETENTION_DAYS_RAW * 86400))

    # Check if there's data to roll up
    COUNT=$(sqlite3 "$DB_PATH" "SELECT COUNT(*) FROM client_traffic WHERE ts < $CUTOFF_RAW;" 2>/dev/null)
    [ -z "$COUNT" ] || [ "$COUNT" -eq 0 ] && return

    logger -t jammonitor "Rolling up $COUNT old client_traffic entries to hourly"

    # Aggregate into hourly buckets (use INSERT OR IGNORE + UPDATE to merge)
    sqlite3 "$DB_PATH" "
        INSERT OR REPLACE INTO client_traffic_hourly (hour_ts, ip, mac, hostname, rx_bytes, tx_bytes)
        SELECT
            (ts / 3600) * 3600 as hour_ts,
            ip,
            mac,
            hostname,
            COALESCE((SELECT rx_bytes FROM client_traffic_hourly WHERE hour_ts = (ts / 3600) * 3600 AND ip = client_traffic.ip), 0) + SUM(rx_bytes),
            COALESCE((SELECT tx_bytes FROM client_traffic_hourly WHERE hour_ts = (ts / 3600) * 3600 AND ip = client_traffic.ip), 0) + SUM(tx_bytes)
        FROM client_traffic
        WHERE ts < $CUTOFF_RAW
        GROUP BY (ts / 3600) * 3600, ip;

        DELETE FROM client_traffic WHERE ts < $CUTOFF_RAW;
    " 2>/dev/null
}

# Dynamic cleanup when disk usage exceeds threshold
dynamic_cleanup() {
    # Get disk usage percentage for /mnt/data
    USAGE=$(df /mnt/data 2>/dev/null | awk 'NR==2 {gsub(/%/,""); print $5}')
    [ -z "$USAGE" ] && return

    if [ "$USAGE" -gt "$DISK_THRESHOLD" ]; then
        # Delete oldest week of hourly rollups
        OLDEST=$(sqlite3 "$DB_PATH" "SELECT MIN(hour_ts) FROM client_traffic_hourly;" 2>/dev/null)
        [ -z "$OLDEST" ] || [ "$OLDEST" = "" ] && return

        CUTOFF=$((OLDEST + 7 * 86400))
        sqlite3 "$DB_PATH" "DELETE FROM client_traffic_hourly WHERE hour_ts < $CUTOFF;" 2>/dev/null
        logger -t jammonitor "Cleaned old hourly rollups, disk was ${USAGE}%"

        # Also run vacuum to reclaim space
        sqlite3 "$DB_PATH" "VACUUM;" 2>/dev/null
    fi
}

# Cleanup old data
cleanup() {
    CUTOFF=$(($(date +%s) - RETENTION_DAYS * 86400))
    sqlite3 "$DB_PATH" "DELETE FROM metrics WHERE ts < $CUTOFF;"
    sqlite3 "$DB_PATH" "DELETE FROM snapshots WHERE ts < $CUTOFF;"

    # Rollup and cleanup per-client traffic
    rollup_client_traffic
    dynamic_cleanup

    # Cleanup old syslog
    if [ -f "${LOG_PATH}.old" ]; then
        OLDTIME=$(stat -c%Y "${LOG_PATH}.old" 2>/dev/null || stat -f%m "${LOG_PATH}.old" 2>/dev/null || echo 0)
        NOW=$(date +%s)
        if [ $((NOW - OLDTIME)) -gt $((RETENTION_DAYS * 86400)) ]; then
            rm -f "${LOG_PATH}.old"
        fi
    fi
}

# Main
init_db

COUNTER=0

while true; do
    # Collect metrics, snapshots, per-client and per-interface traffic every minute
    collect_metrics
    collect_slow
    collect_client_traffic
    collect_interface_traffic
    stream_syslog

    COUNTER=$((COUNTER + 1))

    # Cleanup every hour (60 iterations)
    if [ $COUNTER -ge 60 ]; then
        cleanup
        COUNTER=0
    fi

    sleep 60
done
